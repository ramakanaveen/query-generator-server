# Temperature Control Migration - Practical Example

## üéØ How to Update Your Existing Code

This document shows **exactly** how to update your `query_generator_node.py` with proper temperature control.

---

## Before: Current Code

```python
# app/services/query_generation/nodes/query_generator_node.py

async def generate_initial_query(state, llm):
    """Generate initial query - CURRENT VERSION"""

    # Build prompt
    prompt = GENERATOR_PROMPT_TEMPLATE.format(
        query=state.query,
        schema=state.query_schema,
        execution_plan=state.execution_plan,
        # ... other fields
    )

    # Invoke LLM - using whatever temperature was set at initialization
    response = await llm.ainvoke(prompt)  # ‚Üê Problem: Uses global temperature (0.2)

    return response.content.strip()
```

**Issues:**
- ‚ùå Uses global temperature (0.2) from config
- ‚ùå Can't control top_p, top_k
- ‚ùå Same settings for generation, refinement, retry
- ‚ùå No way to escalate on failures

---

## After: Enhanced Code

### Option 1: Simple Update (Recommended)

```python
# app/services/query_generation/nodes/query_generator_node.py

from app.services.llm_provider import llm_provider

async def generate_initial_query(state, llm):
    """
    Generate initial query with optimal temperature.

    Uses temp=0.0, top_p=0.85, top_k=5 for maximum precision.
    """

    # Get optimized model for query generation
    # This overrides whatever was passed in 'llm' parameter
    optimized_llm = llm_provider.get_for_query_generation(
        model_name="gemini"  # or "claude"
    )

    # Build prompt (same as before)
    prompt = GENERATOR_PROMPT_TEMPLATE.format(
        query=state.query,
        schema=state.query_schema,
        execution_plan=state.execution_plan,
        # ... other fields
    )

    # Invoke with optimized model
    response = await optimized_llm.ainvoke(prompt)

    return response.content.strip()
```

### Option 2: More Explicit

```python
async def generate_initial_query(state, llm):
    """Generate initial query with explicit temperature control"""

    # Get model with specific parameters
    optimized_llm = llm_provider.get_model(
        model_name="gemini",
        temperature=0.0,   # Zero hallucinations
        top_p=0.85,        # Focused nucleus sampling
        top_k=5            # Only top 5 most likely tokens
    )

    prompt = GENERATOR_PROMPT_TEMPLATE.format(...)

    response = await optimized_llm.ainvoke(prompt)

    return response.content.strip()
```

---

## Complete Node Update Example

Here's the full before/after for your actual file:

### BEFORE: generate_initial_query

```python
async def generate_initial_query(state, llm):
    """Standard query generation."""
    try:
        # Retrieve glossary
        glossary = await get_glossary_from_directives(state.directives)

        # Get complexity guidance
        complexity_instructions = get_complexity_guidance(state.query_complexity)

        # Format few-shot examples
        examples_text = format_few_shot_examples(state.few_shot_examples)

        # Build prompt
        prompt = GENERATOR_PROMPT_TEMPLATE.format(
            query=state.query,
            database_type=state.database_type,
            entities=state.entities,
            directives=state.directives,
            glossary=glossary,
            schema=state.query_schema,
            examples=examples_text,
            complexity=state.query_complexity,
            complexity_guidance=complexity_instructions,
            execution_plan=state.execution_plan or "Not specified",
            conversation_history=format_conversation_history(state.conversation_history)
        )

        # Generate query
        response = await llm.ainvoke(prompt)  # ‚Üê Uses global temp
        return response.content.strip()

    except Exception as e:
        logger.error(f"Error in initial query generation: {str(e)}")
        return f"// Error: {str(e)}"
```

### AFTER: generate_initial_query (Enhanced)

```python
from app.services.llm_provider import llm_provider

async def generate_initial_query(state, llm):
    """
    Enhanced query generation with optimal temperature.

    Temperature: 0.0 (deterministic)
    top_p: 0.85 (focused sampling)
    top_k: 5 (minimal hallucination risk)
    """
    try:
        # NEW: Get optimized model for query generation
        # Overrides global settings for maximum precision
        query_llm = llm_provider.get_for_query_generation("gemini")

        # Retrieve glossary (same as before)
        glossary = await get_glossary_from_directives(state.directives)

        # Get complexity guidance (same as before)
        complexity_instructions = get_complexity_guidance(state.query_complexity)

        # Format few-shot examples (same as before)
        examples_text = format_few_shot_examples(state.few_shot_examples)

        # Build prompt (same as before)
        prompt = GENERATOR_PROMPT_TEMPLATE.format(
            query=state.query,
            database_type=state.database_type,
            entities=state.entities,
            directives=state.directives,
            glossary=glossary,
            schema=state.query_schema,
            examples=examples_text,
            complexity=state.query_complexity,
            complexity_guidance=complexity_instructions,
            execution_plan=state.execution_plan or "Not specified",
            conversation_history=format_conversation_history(state.conversation_history)
        )

        # NEW: Generate with optimized model
        response = await query_llm.ainvoke(prompt)

        # NEW: Log temperature used for monitoring
        logger.info(
            f"Query generated with temp=0.0, top_p=0.85, "
            f"query_length={len(response.content)}"
        )

        return response.content.strip()

    except Exception as e:
        logger.error(f"Error in initial query generation: {str(e)}")
        return f"// Error: {str(e)}"
```

---

## Update Refinement Functions

### BEFORE: generate_refinement_query

```python
async def generate_refinement_query(state, llm):
    """Refine query based on validation feedback."""

    refinement_prompt = ENHANCED_REFINER_PROMPT_TEMPLATE.format(
        query=state.query,
        schema=state.query_schema,
        generated_query=state.generated_query,
        validation_errors="\n".join(state.validation_errors),
        refinement_guidance=state.refinement_guidance
    )

    response = await llm.ainvoke(refinement_prompt)  # ‚Üê Uses global temp
    return response.content.strip()
```

### AFTER: generate_refinement_query (Enhanced)

```python
from app.services.llm_provider import llm_provider

async def generate_refinement_query(state, llm):
    """
    Refine query with slightly higher temperature.

    Temperature: 0.3 (allows creative fixes)
    top_p: 0.9 (balanced sampling)
    top_k: 20 (more alternatives)
    """

    # NEW: Use refinement-optimized model
    # Needs slightly more creativity to find fixes
    refine_llm = llm_provider.get_for_refinement("gemini")

    refinement_prompt = ENHANCED_REFINER_PROMPT_TEMPLATE.format(
        query=state.query,
        schema=state.query_schema,
        generated_query=state.generated_query,
        validation_errors="\n".join(state.validation_errors),
        refinement_guidance=state.refinement_guidance
    )

    # NEW: Generate with refinement-optimized model
    response = await refine_llm.ainvoke(refinement_prompt)

    logger.info("Query refined with temp=0.3 for creative problem-solving")

    return response.content.strip()
```

---

## Update Retry Functions

### BEFORE: generate_retry_query

```python
async def generate_retry_query(state, llm):
    """Generate improved query based on user feedback."""

    retry_prompt = RETRY_GENERATION_PROMPT.format(
        original_query=state.query,
        schema=state.query_schema,
        generated_query=state.generated_query,
        user_feedback=state.user_feedback,
        feedback_trail=format_feedback_trail(state.feedback_trail)
    )

    response = await llm.ainvoke(retry_prompt)  # ‚Üê Uses global temp
    return response.content.strip()
```

### AFTER: generate_retry_query (Enhanced)

```python
from app.services.llm_provider import llm_provider

async def generate_retry_query(state, llm):
    """
    Generate improved query with escalated temperature.

    Temperature: 0.4 (explores alternatives)
    top_p: 0.95 (broader sampling)
    top_k: 30 (more creative options)
    """

    # NEW: Use retry-optimized model
    # Higher temperature to explore different approaches
    retry_llm = llm_provider.get_for_retry("gemini")

    retry_prompt = RETRY_GENERATION_PROMPT.format(
        original_query=state.query,
        schema=state.query_schema,
        generated_query=state.generated_query,
        user_feedback=state.user_feedback,
        feedback_trail=format_feedback_trail(state.feedback_trail)
    )

    # NEW: Generate with retry-optimized model
    response = await retry_llm.ainvoke(retry_prompt)

    logger.info(
        f"Retry query generated with temp=0.4 "
        f"(attempt {state.retry_count + 1})"
    )

    return response.content.strip()
```

---

## Advanced: Adaptive Temperature Based on Retry Count

```python
async def generate_retry_query_adaptive(state, llm):
    """
    Generate retry query with temperature escalation.

    Attempt 1: temp=0.0 (try deterministic first)
    Attempt 2: temp=0.2 (add slight variation)
    Attempt 3: temp=0.4 (explore alternatives)
    """

    # Calculate temperature based on retry count
    retry_count = state.retry_count or 0

    if retry_count == 0:
        temperature = 0.0  # First retry: try deterministic
    elif retry_count == 1:
        temperature = 0.2  # Second retry: slight variation
    else:
        temperature = 0.4  # Third+ retry: explore alternatives

    # Get model with escalating temperature
    retry_llm = llm_provider.get_model(
        model_name="gemini",
        temperature=temperature,
        top_p=0.85 + (retry_count * 0.05),  # Gradually increase top_p
        top_k=5 + (retry_count * 10)  # Gradually increase top_k
    )

    logger.info(
        f"Retry attempt {retry_count + 1} with "
        f"temp={temperature}, top_p={0.85 + (retry_count * 0.05)}"
    )

    retry_prompt = RETRY_GENERATION_PROMPT.format(...)

    response = await retry_llm.ainvoke(retry_prompt)

    return response.content.strip()
```

---

## Update Other Nodes

### Intent Classifier

```python
# app/services/query_generation/nodes/intent_classifier.py

from app.services.llm_provider import llm_provider

async def classify_intent(state):
    """Classify user intent with deterministic model."""

    # Use deterministic model for classification
    intent_llm = llm_provider.get_model(
        model_name="gemini",
        stage="INTENT_CLASSIFICATION"  # temp=0.0, top_p=0.9, top_k=10
    )

    prompt = build_intent_prompt(state.query)
    response = await intent_llm.ainvoke(prompt)

    # Parse response
    intent_type = parse_intent(response.content)

    state.intent_type = intent_type
    return state
```

### Intelligent Analyzer

```python
# app/services/query_generation/nodes/intelligent_analyzer.py

from app.services.llm_provider import llm_provider

async def intelligent_analyze_query(state):
    """Analyze query with focused creativity."""

    # Use analysis-optimized model
    analysis_llm = llm_provider.get_model(
        model_name="gemini",
        stage="INTELLIGENT_ANALYSIS"  # temp=0.2, top_p=0.9, top_k=20
    )

    prompt = build_analysis_prompt(state)
    response = await analysis_llm.ainvoke(prompt)

    # Parse analysis
    analysis = parse_analysis(response.content)

    state.query_complexity = analysis['complexity']
    state.execution_plan = analysis['plan']

    return state
```

---

## Testing Your Changes

### Test Script

Create `scripts/test_temperature_migration.py`:

```python
import asyncio
from app.services.llm_provider import llm_provider

async def test_migration():
    """Test that temperature control is working"""

    print("Testing Temperature Control Migration")
    print("=" * 60)

    # Test 1: Query Generation
    print("\n1. Query Generation (should be temp=0.0)")
    gen_llm = llm_provider.get_for_query_generation("gemini")
    print(f"   Temperature: {gen_llm.temperature}")
    print(f"   top_p: {gen_llm.top_p}")
    print(f"   Expected: temp=0.0, top_p=0.85")

    # Test 2: Refinement
    print("\n2. Query Refinement (should be temp=0.3)")
    ref_llm = llm_provider.get_for_refinement("gemini")
    print(f"   Temperature: {ref_llm.temperature}")
    print(f"   top_p: {ref_llm.top_p}")
    print(f"   Expected: temp=0.3, top_p=0.9")

    # Test 3: Retry
    print("\n3. Retry with Feedback (should be temp=0.4)")
    retry_llm = llm_provider.get_for_retry("gemini")
    print(f"   Temperature: {retry_llm.temperature}")
    print(f"   top_p: {retry_llm.top_p}")
    print(f"   Expected: temp=0.4, top_p=0.95")

    # Test 4: Custom
    print("\n4. Custom Parameters")
    custom_llm = llm_provider.get_model(
        "gemini",
        temperature=0.15,
        top_p=0.88,
        top_k=7
    )
    print(f"   Temperature: {custom_llm.temperature}")
    print(f"   top_p: {custom_llm.top_p}")
    print(f"   Expected: temp=0.15, top_p=0.88")

    print("\n" + "=" * 60)
    print("‚úÖ Migration test complete!")

if __name__ == "__main__":
    asyncio.run(test_migration())
```

Run it:
```bash
python scripts/test_temperature_migration.py
```

---

## Migration Checklist

### Files to Update

- [ ] `app/services/llm_provider.py` ‚Üí Replace with enhanced version
- [ ] `app/services/query_generation/nodes/query_generator_node.py`
  - [ ] Update `generate_initial_query()`
  - [ ] Update `generate_refinement_query()`
  - [ ] Update `generate_retry_query()`
  - [ ] Update all LLM guidance functions
- [ ] `app/services/query_generation/nodes/intent_classifier.py`
  - [ ] Update `classify_intent()`
- [ ] `app/services/query_generation/nodes/intelligent_analyzer.py`
  - [ ] Update `intelligent_analyze_query()`
- [ ] `app/services/query_generation/nodes/query_validator.py`
  - [ ] Update `validate_query()`

### Testing

- [ ] Run temperature migration test
- [ ] Test query generation with temp=0.0
- [ ] Test refinement with temp=0.3
- [ ] Test retry with temp=0.4
- [ ] Compare before/after hallucination rates
- [ ] A/B test in production

---

## Summary

**Key Changes:**

1. Import: `from app.services.llm_provider import llm_provider`
2. Replace: `response = await llm.ainvoke(prompt)`
3. With: `optimized_llm = llm_provider.get_for_query_generation("gemini")`
4. Then: `response = await optimized_llm.ainvoke(prompt)`

**Expected Impact:**

- ‚úÖ Hallucinations: ‚Üì 80-90%
- ‚úÖ Consistency: ‚Üë 40%+
- ‚úÖ First-try success: ‚Üë 20%+
- ‚úÖ Syntax errors: ‚Üì 70%+

---

**Questions?** See `TEMPERATURE_CONTROL_GUIDE.md` for full details.