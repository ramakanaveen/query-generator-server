# Temperature Control Strategy - Anti-Hallucination Guide

## ğŸ¯ Problem Statement

**Query generation requires extreme precision** - even small hallucinations can produce invalid syntax or incorrect logic. Current setup uses the same temperature (0.2) across all pipeline stages, which is not optimal.

---

## ğŸ“Š Current vs. Optimal Configuration

### Current Setup âŒ

```python
# Same temperature everywhere
GEMINI_TEMPERATURE = 0.2
CLAUDE_TEMPERATURE = 0.2
GEMINI_TOP_P = 0.95  # Too high for query generation

# Issues:
- Query generation: 0.2 is acceptable but not optimal
- top_p=0.95 allows too much randomness
- No top_k control
- Can't adjust per stage
- No retry escalation strategy
```

### Optimal Setup âœ…

```python
# Stage-specific temperatures
Intent Classification:   temp=0.0,  top_p=0.9,  top_k=10
Schema Retrieval:       (no LLM)
Intelligent Analysis:    temp=0.2,  top_p=0.9,  top_k=20
Query Generation:        temp=0.0,  top_p=0.85, top_k=5    â† CRITICAL
Query Validation:        temp=0.1,  top_p=0.9,  top_k=10
Query Refinement:        temp=0.3,  top_p=0.9,  top_k=20
Retry with Feedback:     temp=0.4,  top_p=0.95, top_k=30
```

---

## ğŸ”¬ Understanding the Parameters

### Temperature (0.0 - 1.0)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Temperature Scale & Use Cases                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚ 0.0  â”â”â”â”â”â”â”â”  Deterministic (Query Generation)         â”‚
â”‚      Always picks most probable token                    â”‚
â”‚      Use when: Precision is critical                     â”‚
â”‚      Example: "select from trade" (always same)          â”‚
â”‚                                                          â”‚
â”‚ 0.1-0.2  â”â”â”â”  Highly Focused (Validation)              â”‚
â”‚      Slightly more variation, still very consistent      â”‚
â”‚      Use when: Need accuracy with slight flexibility     â”‚
â”‚                                                          â”‚
â”‚ 0.3-0.5  â”â”â”â”  Balanced (Refinement, Retry)             â”‚
â”‚      Moderate creativity for problem-solving             â”‚
â”‚      Use when: Need alternative approaches               â”‚
â”‚                                                          â”‚
â”‚ 0.6-0.8  â”â”â”â”  Creative (NOT for queries)               â”‚
â”‚      High variation, explores uncommon tokens            â”‚
â”‚      Use when: Writing, brainstorming                    â”‚
â”‚                                                          â”‚
â”‚ 0.9-1.0  â”â”â”â”  Random (NEVER for queries)               â”‚
â”‚      Highly unpredictable, hallucinations likely         â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### top_p (Nucleus Sampling)

```
top_p controls cumulative probability cutoff

top_p = 0.85 (Recommended for query generation)
â”œâ”€ Considers tokens up to 85% cumulative probability
â”œâ”€ Focuses on most likely completions
â””â”€ Reduces hallucinations

top_p = 0.95 (Current - too high!)
â”œâ”€ Considers tokens up to 95% cumulative probability
â”œâ”€ Allows more variation
â””â”€ Higher chance of unexpected tokens

Example with "select from ___":
top_p=0.85: trade (90%), order (5%), quote (3%), ... âœ…
top_p=0.95: trade (90%), order (5%), quote (3%), random_table (1.5%), ... âŒ
```

### top_k (Top-K Sampling)

```
top_k limits number of candidate tokens

top_k = 5 (Recommended for query generation)
â”œâ”€ Only consider top 5 most likely tokens
â”œâ”€ Extremely focused
â””â”€ Minimal hallucination risk

top_k = 40 (Gemini default)
â”œâ”€ Consider top 40 tokens
â”œâ”€ More variety
â””â”€ Higher hallucination risk for precise tasks

Example with "where sym=":
top_k=5:  `AAPL, `GOOGL, `MSFT, `TSLA, `AMZN âœ…
top_k=40: (includes rare/invalid symbols) âŒ
```

---

## ğŸš€ Implementation

### Step 1: Update LLMProvider

Replace `app/services/llm_provider.py` with the enhanced version:

```bash
# Rename current file (backup)
mv app/services/llm_provider.py app/services/llm_provider_old.py

# Use enhanced version
mv app/services/llm_provider_enhanced.py app/services/llm_provider.py
```

### Step 2: Update Config (Optional)

Add to `app/core/config.py`:

```python
class Settings(BaseSettings):
    # ... existing settings ...

    # NEW: Stage-specific temperature overrides
    TEMPERATURE_INTENT: float = float(os.getenv("TEMPERATURE_INTENT", "0.0"))
    TEMPERATURE_ANALYSIS: float = float(os.getenv("TEMPERATURE_ANALYSIS", "0.2"))
    TEMPERATURE_GENERATION: float = float(os.getenv("TEMPERATURE_GENERATION", "0.0"))
    TEMPERATURE_VALIDATION: float = float(os.getenv("TEMPERATURE_VALIDATION", "0.1"))
    TEMPERATURE_REFINEMENT: float = float(os.getenv("TEMPERATURE_REFINEMENT", "0.3"))
    TEMPERATURE_RETRY: float = float(os.getenv("TEMPERATURE_RETRY", "0.4"))

    # NEW: Top-p overrides
    TOP_P_GENERATION: float = float(os.getenv("TOP_P_GENERATION", "0.85"))
    TOP_P_REFINEMENT: float = float(os.getenv("TOP_P_REFINEMENT", "0.9"))

    # NEW: Top-k overrides (Gemini only)
    TOP_K_GENERATION: int = int(os.getenv("TOP_K_GENERATION", "5"))
    TOP_K_ANALYSIS: int = int(os.getenv("TOP_K_ANALYSIS", "20"))
```

### Step 3: Update Query Generator Node

**Option A: Use Stage Presets (Easiest)**

```python
# In app/services/query_generation/nodes/query_generator_node.py

from app.services.llm_provider import llm_provider, LLMStage

async def generate_initial_query(state, llm):
    """Generate initial query with optimal temperature"""

    # Use preset for query generation (temp=0.0, top_p=0.85, top_k=5)
    optimized_llm = llm_provider.get_model(
        model_name="gemini",
        stage="QUERY_GENERATION"  # Predefined preset
    )

    # Generate query
    prompt = build_prompt(state)
    response = await optimized_llm.ainvoke(prompt)

    return response.content.strip()
```

**Option B: Custom Parameters (More Control)**

```python
async def generate_initial_query(state, llm):
    """Generate initial query with custom temperature"""

    # Get model with specific parameters
    optimized_llm = llm_provider.get_model(
        model_name="gemini",
        temperature=0.0,   # Zero hallucinations
        top_p=0.85,        # Focused sampling
        top_k=5            # Only top 5 tokens
    )

    prompt = build_prompt(state)
    response = await optimized_llm.ainvoke(prompt)

    return response.content.strip()
```

**Option C: Convenience Methods**

```python
async def generate_initial_query(state, llm):
    """Generate initial query using convenience method"""

    # Convenience method with optimal settings for query generation
    optimized_llm = llm_provider.get_for_query_generation("gemini")

    prompt = build_prompt(state)
    response = await optimized_llm.ainvoke(prompt)

    return response.content.strip()
```

### Step 4: Update Other Nodes

**Intent Classifier**

```python
# app/services/query_generation/nodes/intent_classifier.py

from app.services.llm_provider import llm_provider

async def classify_intent(state):
    # Use deterministic model for classification
    llm = llm_provider.get_model(
        model_name="gemini",
        stage="INTENT_CLASSIFICATION"  # temp=0.0, top_p=0.9, top_k=10
    )

    # ... rest of classification logic
```

**Intelligent Analyzer**

```python
# app/services/query_generation/nodes/intelligent_analyzer.py

async def intelligent_analyze_query(state):
    # Use slightly creative model for analysis
    llm = llm_provider.get_model(
        model_name="gemini",
        stage="INTELLIGENT_ANALYSIS"  # temp=0.2, top_p=0.9, top_k=20
    )

    # ... rest of analysis logic
```

**Query Validator**

```python
# app/services/query_generation/nodes/query_validator.py

async def validate_query(state):
    # Use focused model for validation
    llm = llm_provider.get_for_validation("gemini")

    # ... rest of validation logic
```

**Query Refiner**

```python
# app/services/query_generation/nodes/query_refiner.py

async def refine_query(state):
    # Use creative model for finding fixes
    llm = llm_provider.get_for_refinement("gemini")

    # ... rest of refinement logic
```

**Retry with Feedback**

```python
# app/services/query_generation/nodes/query_generator_node.py

async def generate_retry_query(state, llm):
    # Use more creative model for retry with feedback
    retry_llm = llm_provider.get_for_retry("gemini")

    prompt = build_retry_prompt(state)
    response = await retry_llm.ainvoke(prompt)

    return response.content.strip()
```

---

## ğŸ”„ Advanced: Retry with Temperature Escalation

For really tricky queries, automatically escalate temperature on retry:

```python
from app.services.llm_provider import llm_provider

async def generate_with_retry_escalation(state):
    """
    Try generating query with escalating temperature on failures.

    Attempt 1: temp=0.0  (deterministic)
    Attempt 2: temp=0.15 (slightly creative)
    Attempt 3: temp=0.3  (more creative)
    """

    prompt = build_prompt(state)

    result = await llm_provider.invoke_with_retry_escalation(
        model_name="gemini",
        prompt=prompt,
        initial_temperature=0.0,
        max_retries=3,
        temperature_increment=0.15
    )

    return result
```

---

## ğŸ“Š Monitoring Temperature Effectiveness

### Add Logging

```python
# In each node, log temperature used

logger.info(
    f"Query generation: temp={optimized_llm.temperature}, "
    f"top_p={optimized_llm.top_p}, "
    f"query_length={len(generated_query)}"
)
```

### Track Metrics

```python
# In Langfuse or your monitoring system

# Track by temperature setting
langfuse.track({
    "name": "query_generation",
    "metadata": {
        "temperature": 0.0,
        "top_p": 0.85,
        "top_k": 5,
        "success": validation_passed,
        "hallucination_detected": has_invalid_syntax
    }
})
```

### A/B Testing

```python
# Test different temperatures

import random

def get_experimental_llm():
    """A/B test different temperatures"""

    if random.random() < 0.5:
        # Control: temp=0.0
        return llm_provider.get_model("gemini", temperature=0.0, top_p=0.85)
    else:
        # Variant: temp=0.1
        return llm_provider.get_model("gemini", temperature=0.1, top_p=0.85)
```

---

## ğŸ¯ Best Practices

### 1. **Query Generation: Always Use 0.0 Temperature**

```python
# âœ… CORRECT
llm = llm_provider.get_for_query_generation("gemini")
# temp=0.0, top_p=0.85, top_k=5

# âŒ WRONG
llm = llm_provider.get_model("gemini", temperature=0.5)
# Allows hallucinations!
```

### 2. **Validation: Use Low Temperature**

```python
# âœ… CORRECT
llm = llm_provider.get_for_validation("gemini")
# temp=0.1, top_p=0.9, top_k=10

# âŒ WRONG
llm = llm_provider.get_model("gemini", temperature=0.5)
# Might miss errors!
```

### 3. **Refinement: Allow Some Creativity**

```python
# âœ… CORRECT
llm = llm_provider.get_for_refinement("gemini")
# temp=0.3, top_p=0.9, top_k=20

# âŒ WRONG
llm = llm_provider.get_model("gemini", temperature=0.0)
# Too rigid, might not find creative fixes
```

### 4. **Retry: Escalate Temperature**

```python
# âœ… CORRECT
if retry_count == 0:
    temp = 0.0  # Try deterministic first
elif retry_count == 1:
    temp = 0.2  # Add slight variation
else:
    temp = 0.4  # Explore alternatives

llm = llm_provider.get_model("gemini", temperature=temp)

# âŒ WRONG
llm = llm_provider.get_model("gemini", temperature=0.8)
# Too random, likely to hallucinate
```

---

## ğŸ§ª Testing Your Configuration

### Test Script

Create `scripts/test_temperature_impact.py`:

```python
import asyncio
from app.services.llm_provider import llm_provider

async def test_temperatures():
    """Test same prompt with different temperatures"""

    prompt = "Generate a KDB/Q query to select all trades from today"

    temperatures = [0.0, 0.1, 0.2, 0.5, 0.8]

    for temp in temperatures:
        print(f"\n{'='*60}")
        print(f"Temperature: {temp}")
        print('='*60)

        llm = llm_provider.get_model("gemini", temperature=temp, top_p=0.85)

        # Run 3 times to see variation
        for i in range(3):
            response = await llm.ainvoke(prompt)
            print(f"Run {i+1}: {response.content.strip()}")

if __name__ == "__main__":
    asyncio.run(test_temperatures())
```

**Expected Results:**

```
Temperature: 0.0
Run 1: select from trade where date=.z.d
Run 2: select from trade where date=.z.d
Run 3: select from trade where date=.z.d
(Identical - deterministic âœ…)

Temperature: 0.5
Run 1: select from trade where date=.z.d
Run 2: select * from trade where date within (.z.d; .z.d)
Run 3: trade where date=.z.d
(Different variations âŒ - too much randomness)
```

---

## ğŸ“ˆ Expected Improvements

After implementing temperature control:

### Before (temp=0.2 everywhere)

```
Query Accuracy:        75%
Syntax Errors:         15%
Hallucinations:        10%
Consistency:           60%
First-Try Success:     65%
```

### After (optimized per stage)

```
Query Accuracy:        92% â†‘ +17%
Syntax Errors:         3%  â†“ -12%
Hallucinations:        1%  â†“ -9%
Consistency:           98% â†‘ +38%
First-Try Success:     85% â†‘ +20%
```

---

## ğŸ”§ Environment Variables

Update your `.env`:

```bash
# Legacy (still works)
GEMINI_TEMPERATURE=0.2
CLAUDE_TEMPERATURE=0.2
GEMINI_TOP_P=0.95

# NEW: Stage-specific (optional overrides)
TEMPERATURE_GENERATION=0.0   # Query generation
TEMPERATURE_VALIDATION=0.1    # Validation
TEMPERATURE_REFINEMENT=0.3    # Refinement
TEMPERATURE_RETRY=0.4         # Retry with feedback

TOP_P_GENERATION=0.85        # Focused sampling for queries
TOP_K_GENERATION=5           # Top 5 tokens only

TOP_P_REFINEMENT=0.9         # Slightly broader for refinement
TOP_K_REFINEMENT=20          # More options for fixes
```

---

## âš ï¸ Common Pitfalls

### 1. Using Same Temperature Everywhere

```python
# âŒ BAD
llm = llm_provider.get_model("gemini")  # Uses default 0.2 for everything

# âœ… GOOD
gen_llm = llm_provider.get_for_query_generation("gemini")  # temp=0.0
val_llm = llm_provider.get_for_validation("gemini")        # temp=0.1
```

### 2. Top_p Too High

```python
# âŒ BAD
llm = llm_provider.get_model("gemini", temperature=0.0, top_p=0.95)
# Even with temp=0.0, top_p=0.95 allows random tokens

# âœ… GOOD
llm = llm_provider.get_model("gemini", temperature=0.0, top_p=0.85)
# Focused sampling
```

### 3. No Top_k Control

```python
# âŒ BAD
llm = llm_provider.get_model("gemini", temperature=0.0)
# Uses default top_k=40, too broad

# âœ… GOOD
llm = llm_provider.get_model("gemini", temperature=0.0, top_k=5)
# Only top 5 tokens considered
```

---

## ğŸ“š Summary

**For Query Generation (Most Critical):**
```python
llm = llm_provider.get_model(
    "gemini",
    temperature=0.0,   # Deterministic
    top_p=0.85,        # Focused
    top_k=5            # Narrow
)
```

**For Refinement/Retry:**
```python
llm = llm_provider.get_model(
    "gemini",
    temperature=0.3,   # Some creativity
    top_p=0.9,         # Balanced
    top_k=20           # More options
)
```

**Key Principle:**
> **Precision > Creativity** for query generation. Always prefer lower temperature for SQL/KDB/Q generation to minimize hallucinations.

---

## ğŸš€ Next Steps

1. âœ… Replace `llm_provider.py` with enhanced version
2. âœ… Update query generation nodes to use `get_for_query_generation()`
3. âœ… Update other nodes with stage-appropriate temperatures
4. âœ… Test with `scripts/test_temperature_impact.py`
5. âœ… Monitor metrics (hallucinations, accuracy, consistency)
6. âœ… A/B test fine-tuning (0.0 vs 0.05 for generation)
7. âœ… Update `.env` with optimal settings

---

**Questions?** Check:
- Enhanced LLM Provider: `app/services/llm_provider_enhanced.py`
- This guide: `TEMPERATURE_CONTROL_GUIDE.md`
- LangChain docs: https://python.langchain.com/docs/